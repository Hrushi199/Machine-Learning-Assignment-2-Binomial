{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90da346e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Using Device: cuda\n",
      "Loading Data...\n",
      "Feature Engineering...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-23 18:19:49,529] A new study created in memory with name: no-name-0073150f-1e71-4a1f-a694-aefee775aee9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Optuna Study (Reproducible)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-23 18:21:06,744] Trial 0 finished with value: 0.7486536856104717 and parameters: {'lr': 0.0005611516415334506, 'dropout': 0.4802857225639665, 'weight_decay': 0.000157029708840554, 'l1_size': 256, 'l2_size': 256, 'l3_size': 256}. Best is trial 0 with value: 0.7486536856104717.\n",
      "[I 2025-11-23 18:22:29,890] Trial 1 finished with value: 0.7462181924817578 and parameters: {'lr': 0.004622589001020831, 'dropout': 0.18493564427131048, 'weight_decay': 3.5113563139704077e-06, 'l1_size': 1024, 'l2_size': 512, 'l3_size': 256}. Best is trial 0 with value: 0.7486536856104717.\n",
      "[I 2025-11-23 18:23:43,758] Trial 2 finished with value: 0.7494691545783992 and parameters: {'lr': 0.000816845589476017, 'dropout': 0.41407038455720546, 'weight_decay': 3.972110727381911e-06, 'l1_size': 512, 'l2_size': 128, 'l3_size': 128}. Best is trial 2 with value: 0.7494691545783992.\n",
      "[I 2025-11-23 18:24:58,971] Trial 3 finished with value: 0.7433880586874138 and parameters: {'lr': 0.0004066563313514797, 'dropout': 0.13906884560255356, 'weight_decay': 0.00011290133559092664, 'l1_size': 1024, 'l2_size': 256, 'l3_size': 64}. Best is trial 2 with value: 0.7494691545783992.\n",
      "[I 2025-11-23 18:26:17,215] Trial 4 finished with value: 0.745007335369665 and parameters: {'lr': 0.0012399967836846098, 'dropout': 0.17394178221021084, 'weight_decay': 0.0008105016126411582, 'l1_size': 512, 'l2_size': 256, 'l3_size': 256}. Best is trial 2 with value: 0.7494691545783992.\n",
      "[I 2025-11-23 18:27:36,821] Trial 5 finished with value: 0.7441310084131985 and parameters: {'lr': 0.0005989003672254305, 'dropout': 0.20853961270955837, 'weight_decay': 0.0003063462210622083, 'l1_size': 1024, 'l2_size': 256, 'l3_size': 64}. Best is trial 2 with value: 0.7494691545783992.\n",
      "[I 2025-11-23 18:28:51,165] Trial 6 finished with value: 0.7416471793504826 and parameters: {'lr': 0.00010257563974185662, 'dropout': 0.42618457138193366, 'weight_decay': 0.00013199942261535007, 'l1_size': 512, 'l2_size': 512, 'l3_size': 64}. Best is trial 2 with value: 0.7494691545783992.\n",
      "[I 2025-11-23 18:30:06,747] Trial 7 finished with value: 0.7454894682224634 and parameters: {'lr': 0.0004187594718900631, 'dropout': 0.23007332881069884, 'weight_decay': 0.0001544608907504709, 'l1_size': 512, 'l2_size': 512, 'l3_size': 128}. Best is trial 2 with value: 0.7494691545783992.\n",
      "[I 2025-11-23 18:31:28,701] Trial 8 finished with value: 0.7450532107039598 and parameters: {'lr': 0.0011103647313054626, 'dropout': 0.27101640734341986, 'weight_decay': 1.1919481947918734e-06, 'l1_size': 1024, 'l2_size': 512, 'l3_size': 256}. Best is trial 2 with value: 0.7494691545783992.\n",
      "[I 2025-11-23 18:33:02,113] Trial 9 finished with value: 0.7450162036370584 and parameters: {'lr': 0.00028681134821030097, 'dropout': 0.1307919639315172, 'weight_decay': 7.400385759087375e-06, 'l1_size': 512, 'l2_size': 256, 'l3_size': 128}. Best is trial 2 with value: 0.7494691545783992.\n",
      "[I 2025-11-23 18:34:34,910] Trial 10 finished with value: 0.7490324668542813 and parameters: {'lr': 0.0035848769372239975, 'dropout': 0.3699188271527112, 'weight_decay': 1.94953750948045e-05, 'l1_size': 256, 'l2_size': 128, 'l3_size': 128}. Best is trial 2 with value: 0.7494691545783992.\n",
      "[I 2025-11-23 18:36:02,143] Trial 11 finished with value: 0.7492274579811815 and parameters: {'lr': 0.0038447860362774786, 'dropout': 0.3551887020844243, 'weight_decay': 1.827643917277538e-05, 'l1_size': 256, 'l2_size': 128, 'l3_size': 128}. Best is trial 2 with value: 0.7494691545783992.\n",
      "[I 2025-11-23 18:37:24,158] Trial 12 finished with value: 0.7489154064181829 and parameters: {'lr': 0.007824723380771288, 'dropout': 0.34624005834785887, 'weight_decay': 2.6190393283008903e-05, 'l1_size': 256, 'l2_size': 128, 'l3_size': 128}. Best is trial 2 with value: 0.7494691545783992.\n",
      "[I 2025-11-23 18:38:33,864] Trial 13 finished with value: 0.7494433871816395 and parameters: {'lr': 0.002259394689518437, 'dropout': 0.385148011446725, 'weight_decay': 7.334484850391938e-06, 'l1_size': 256, 'l2_size': 128, 'l3_size': 128}. Best is trial 2 with value: 0.7494691545783992.\n",
      "[I 2025-11-23 18:39:41,856] Trial 14 finished with value: 0.7494846453599349 and parameters: {'lr': 0.001800055478731078, 'dropout': 0.43045878126021486, 'weight_decay': 4.210060503334372e-06, 'l1_size': 512, 'l2_size': 128, 'l3_size': 128}. Best is trial 14 with value: 0.7494846453599349.\n",
      "[I 2025-11-23 18:40:51,264] Trial 15 finished with value: 0.7496890702227816 and parameters: {'lr': 0.001973550936336413, 'dropout': 0.489628049668195, 'weight_decay': 1.1930220880472108e-06, 'l1_size': 512, 'l2_size': 128, 'l3_size': 128}. Best is trial 15 with value: 0.7496890702227816.\n",
      "[I 2025-11-23 18:42:10,696] Trial 16 finished with value: 0.7496095667591922 and parameters: {'lr': 0.00171745877836491, 'dropout': 0.486135178426182, 'weight_decay': 1.1066368131661548e-06, 'l1_size': 512, 'l2_size': 128, 'l3_size': 128}. Best is trial 15 with value: 0.7496890702227816.\n",
      "[I 2025-11-23 18:43:44,403] Trial 17 finished with value: 0.7501704598235929 and parameters: {'lr': 0.001885274701655737, 'dropout': 0.49606564079850074, 'weight_decay': 1.3802926743044478e-06, 'l1_size': 512, 'l2_size': 128, 'l3_size': 128}. Best is trial 17 with value: 0.7501704598235929.\n",
      "[I 2025-11-23 18:44:56,216] Trial 18 finished with value: 0.7489511119085337 and parameters: {'lr': 0.007501054462493065, 'dropout': 0.48921829308474996, 'weight_decay': 2.0473651024087276e-06, 'l1_size': 512, 'l2_size': 128, 'l3_size': 128}. Best is trial 17 with value: 0.7501704598235929.\n",
      "[I 2025-11-23 18:45:47,795] Trial 19 finished with value: 0.7484319044548775 and parameters: {'lr': 0.0025815325463280888, 'dropout': 0.30730254041170946, 'weight_decay': 4.6883716590704525e-05, 'l1_size': 512, 'l2_size': 128, 'l3_size': 64}. Best is trial 17 with value: 0.7501704598235929.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "âœ… BEST MACRO F1: 0.7502\n",
      "==================================================\n",
      "BEST PARAMS: {'lr': 0.001885274701655737, 'dropout': 0.49606564079850074, 'weight_decay': 1.3802926743044478e-06, 'l1_size': 512, 'l2_size': 128, 'l3_size': 128}\n",
      "\n",
      "==================================================\n",
      "ðŸš€ STARTING FINAL TRAINING WITH BEST PARAMS\n",
      "==================================================\n",
      "\n",
      "âš¡ FOLD 1/5\n",
      "  Early stopping at epoch 19\n",
      "  Best Val Loss: 0.4804 | F1 (0.5): 0.7527\n",
      "\n",
      "âš¡ FOLD 2/5\n",
      "  Early stopping at epoch 18\n",
      "  Best Val Loss: 0.4795 | F1 (0.5): 0.7503\n",
      "\n",
      "âš¡ FOLD 3/5\n",
      "  Early stopping at epoch 27\n",
      "  Best Val Loss: 0.4785 | F1 (0.5): 0.7476\n",
      "\n",
      "âš¡ FOLD 4/5\n",
      "  Early stopping at epoch 22\n",
      "  Best Val Loss: 0.4870 | F1 (0.5): 0.7490\n",
      "\n",
      "âš¡ FOLD 5/5\n",
      "  Early stopping at epoch 26\n",
      "  Best Val Loss: 0.4855 | F1 (0.5): 0.7399\n",
      "\n",
      "Optimizing Threshold on OOF Predictions...\n",
      "Global OOF Macro F1: 0.7487 at Threshold: 0.485\n",
      "Saved 'submission_nn_optuna_final.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import optuna\n",
    "import random\n",
    "import os\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# 1. REPRODUCIBILITY SETUP (CRITICAL)\n",
    "# ============================================================================\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸš€ Using Device: {device}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. PREPROCESSING\n",
    "# ============================================================================\n",
    "print(\"Loading Data...\")\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Handle IDs\n",
    "test_ids = df_test['founder_id'].copy()\n",
    "df_train.drop('founder_id', axis=1, inplace=True)\n",
    "df_test.drop('founder_id', axis=1, inplace=True)\n",
    "\n",
    "# Target Map\n",
    "df_train['retention_status'] = df_train['retention_status'].map({'Left': 1, 'Stayed': 0})\n",
    "y = df_train['retention_status'].values\n",
    "X = df_train.drop('retention_status', axis=1)\n",
    "\n",
    "def clean_text(df):\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = df[col].str.replace('â€™', \"'\").str.replace('â€˜', \"'\")\n",
    "    return df\n",
    "\n",
    "X = clean_text(X)\n",
    "X_test = clean_text(df_test)\n",
    "\n",
    "def process_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Binary Mapping\n",
    "    bin_cols = ['working_overtime', 'remote_operations', 'leadership_scope', 'innovation_support']\n",
    "    for c in bin_cols:\n",
    "        df[c] = df[c].map({'Yes': 1, 'No': 0}).fillna(0).astype(int)\n",
    "    \n",
    "    # 2. Ratios & Interactions\n",
    "    def safe_ratio(a, b): return np.where((b!=0) & (~pd.isna(b)), a/b, 0)\n",
    "    \n",
    "    df['funding_velocity'] = safe_ratio(df['funding_rounds_led'], df['years_since_founding'])\n",
    "    size_map = {'Small': 1, 'Medium': 2, 'Large': 3, 'Unknown': 0}\n",
    "    df['team_complexity'] = df['remote_operations'] * df['team_size_category'].map(size_map).fillna(0)\n",
    "    df['revenue_per_year'] = safe_ratio(df['monthly_revenue_generated'], df['years_since_founding'])\n",
    "    df['founder_tenure_ratio'] = safe_ratio(df['years_with_startup'], df['years_since_founding'])\n",
    "    df['is_married'] = (df['personal_status'] == 'Married').astype(int)\n",
    "    df['family_burden'] = df['num_dependents'].fillna(0) * df['is_married']\n",
    "\n",
    "    # 3. Ordinal Scores\n",
    "    bal_map = {'Poor':1, 'Fair':2, 'Good':3, 'Excellent':4, 'Unknown': 2}\n",
    "    perf_map = {'Poor':1, 'Average':2, 'Good':3, 'Excellent':4, 'Unknown': 2}\n",
    "    rep_map  = {'Poor':1, 'Fair':2, 'Good':3, 'Excellent':4, 'Unknown': 2}\n",
    "    sat_map  = {'Low':1, 'Medium':2, 'High':3, 'Very High':4, 'Unknown': 2}\n",
    "\n",
    "    df['work_pressure'] = df['working_overtime'] * (5 - df['work_life_balance_rating'].fillna('Unknown').map(bal_map))\n",
    "    df['success_score'] = (df['startup_performance_rating'].map(perf_map).fillna(2) + \\\n",
    "                           df['startup_reputation'].map(rep_map).fillna(2) + \\\n",
    "                           df['venture_satisfaction'].map(sat_map).fillna(2)) / 3\n",
    "    df['burnout_index'] = df['work_pressure'] * df['family_burden']\n",
    "    \n",
    "    # 4. Binning\n",
    "    df['revenue_binned'] = pd.cut(df['monthly_revenue_generated'], bins=[-1, 5000, 8000, np.inf], labels=[0,1,2]).astype(float)\n",
    "    \n",
    "    # 5. Clean Missing Strings\n",
    "    cat_cols = ['founder_gender', 'founder_role', 'personal_status', 'team_size_category', 'founder_visibility', \n",
    "                'education_background', 'startup_stage', 'work_life_balance_rating', \n",
    "                'startup_performance_rating', 'startup_reputation', 'venture_satisfaction']\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].fillna('Unknown').astype(str)\n",
    "        \n",
    "    return df\n",
    "\n",
    "print(\"Feature Engineering...\")\n",
    "X_proc = process_features(X)\n",
    "X_test_proc = process_features(X_test)\n",
    "\n",
    "# --- NN Specific Prep ---\n",
    "cat_cols = ['founder_gender', 'founder_role', 'personal_status', 'team_size_category', 'founder_visibility', \n",
    "            'education_background', 'startup_stage', 'work_life_balance_rating', \n",
    "            'startup_performance_rating', 'startup_reputation', 'venture_satisfaction']\n",
    "num_cols = [c for c in X_proc.columns if c not in cat_cols]\n",
    "\n",
    "# 1. Fill Numerical NaNs\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_proc[num_cols] = imputer.fit_transform(X_proc[num_cols])\n",
    "X_test_proc[num_cols] = imputer.transform(X_test_proc[num_cols])\n",
    "\n",
    "# 2. Scale Numericals\n",
    "scaler = StandardScaler()\n",
    "X_proc[num_cols] = scaler.fit_transform(X_proc[num_cols])\n",
    "X_test_proc[num_cols] = scaler.transform(X_test_proc[num_cols])\n",
    "\n",
    "# 3. Label Encode Categoricals for Embeddings\n",
    "cat_dims = []\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    full_list = pd.concat([X_proc[col], X_test_proc[col]], axis=0).astype(str)\n",
    "    le.fit(full_list)\n",
    "    \n",
    "    X_proc[col] = le.transform(X_proc[col].astype(str))\n",
    "    X_test_proc[col] = le.transform(X_test_proc[col].astype(str))\n",
    "    \n",
    "    vocab = len(le.classes_) + 1 \n",
    "    emb_dim = min(50, (vocab + 1) // 2)\n",
    "    cat_dims.append((vocab, emb_dim))\n",
    "\n",
    "# Convert to Numpy\n",
    "X_cat = X_proc[cat_cols].values.astype(np.int64)\n",
    "X_num = X_proc[num_cols].values.astype(np.float32)\n",
    "# Add Test Numpy arrays for final inference\n",
    "X_test_cat = X_test_proc[cat_cols].values.astype(np.int64)\n",
    "X_test_num = X_test_proc[num_cols].values.astype(np.float32)\n",
    "\n",
    "# ============================================================================\n",
    "# 3. MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "class FounderDataset(Dataset):\n",
    "    def __init__(self, cat_data, num_data, targets=None):\n",
    "        self.cat_data = torch.tensor(cat_data, dtype=torch.long)\n",
    "        self.num_data = torch.tensor(num_data, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32).unsqueeze(1) if targets is not None else None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.cat_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.targets is not None:\n",
    "            return self.cat_data[idx], self.num_data[idx], self.targets[idx]\n",
    "        return self.cat_data[idx], self.num_data[idx]\n",
    "\n",
    "class GoatedMLP(nn.Module):\n",
    "    def __init__(self, embedding_dims, n_num, dropout, l1_size, l2_size, l3_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeddings = nn.ModuleList([nn.Embedding(v, d) for v, d in embedding_dims])\n",
    "        self.n_emb_out = sum(d for v, d in embedding_dims)\n",
    "        self.bn_num = nn.BatchNorm1d(n_num)\n",
    "        in_dim = self.n_emb_out + n_num\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(in_dim, l1_size),\n",
    "            nn.BatchNorm1d(l1_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(l1_size, l2_size),\n",
    "            nn.BatchNorm1d(l2_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(l2_size, l3_size),\n",
    "            nn.BatchNorm1d(l3_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout/2)\n",
    "        )\n",
    "        self.head = nn.Linear(l3_size, 1)\n",
    "\n",
    "    def forward(self, x_cat, x_num):\n",
    "        emb_out = [emb(x_cat[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        x_emb = torch.cat(emb_out, 1)\n",
    "        x_num = self.bn_num(x_num)\n",
    "        x = torch.cat([x_emb, x_num], 1)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return self.head(x)\n",
    "\n",
    "# ============================================================================\n",
    "# 4. OPTUNA OBJECTIVE\n",
    "# ============================================================================\n",
    "def objective(trial):\n",
    "    seed_everything(SEED)\n",
    "    \n",
    "    params = {\n",
    "        'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "        'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True),\n",
    "        'l1_size': trial.suggest_categorical('l1_size', [256, 512, 1024]),\n",
    "        'l2_size': trial.suggest_categorical('l2_size', [128, 256, 512]),\n",
    "        'l3_size': trial.suggest_categorical('l3_size', [64, 128, 256]),\n",
    "        'batch_size': 512,\n",
    "    }\n",
    "    \n",
    "    if params['l2_size'] > params['l1_size'] or params['l3_size'] > params['l2_size']:\n",
    "        pass\n",
    "\n",
    "    n_splits = 5\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    fold_f1_scores = []\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_cat, y)):\n",
    "        X_cat_tr, X_cat_val = X_cat[train_idx], X_cat[val_idx]\n",
    "        X_num_tr, X_num_val = X_num[train_idx], X_num[val_idx]\n",
    "        y_tr, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        train_ds = FounderDataset(X_cat_tr, X_num_tr, y_tr)\n",
    "        val_ds = FounderDataset(X_cat_val, X_num_val, y_val)\n",
    "        \n",
    "        train_loader = DataLoader(train_ds, batch_size=params['batch_size'], shuffle=True, num_workers=0)\n",
    "        val_loader = DataLoader(val_ds, batch_size=params['batch_size']*2, shuffle=False, num_workers=0)\n",
    "        \n",
    "        model = GoatedMLP(\n",
    "            embedding_dims=cat_dims, \n",
    "            n_num=X_num.shape[1],\n",
    "            dropout=params['dropout'],\n",
    "            l1_size=params['l1_size'],\n",
    "            l2_size=params['l2_size'],\n",
    "            l3_size=params['l3_size']\n",
    "        ).to(device)\n",
    "        \n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "        epochs = 15 \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for cat, num, target in train_loader:\n",
    "                cat, num, target = cat.to(device), num.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(cat, num)\n",
    "                loss = criterion(logits, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_targets_all = []\n",
    "        with torch.no_grad():\n",
    "            for cat, num, target in val_loader:\n",
    "                cat, num, target = cat.to(device), num.to(device), target.to(device)\n",
    "                logits = model(cat, num)\n",
    "                val_preds.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "                val_targets_all.extend(target.cpu().numpy())\n",
    "        \n",
    "        best_f1_fold = 0\n",
    "        preds_arr = np.array(val_preds)\n",
    "        targets_arr = np.array(val_targets_all)\n",
    "        for t in np.arange(0.3, 0.7, 0.05):\n",
    "            p = (preds_arr >= t).astype(int)\n",
    "            f1 = f1_score(targets_arr, p, average='macro')\n",
    "            if f1 > best_f1_fold:\n",
    "                best_f1_fold = f1\n",
    "        fold_f1_scores.append(best_f1_fold)\n",
    "        \n",
    "    return np.mean(fold_f1_scores)\n",
    "\n",
    "# ============================================================================\n",
    "# 5. RUN OPTIMIZATION\n",
    "# ============================================================================\n",
    "print(\"\\nStarting Optuna Study (Reproducible)...\")\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "study = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=20) \n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"âœ… BEST MACRO F1: {study.best_value:.4f}\")\n",
    "print(\"=\"*50)\n",
    "best_params = study.best_params\n",
    "print(\"BEST PARAMS:\", best_params)\n",
    "\n",
    "# ============================================================================\n",
    "# 6. FINAL TRAINING & SUBMISSION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸš€ STARTING FINAL TRAINING WITH BEST PARAMS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_preds = np.zeros(len(X_cat))\n",
    "test_preds_accum = np.zeros(len(X_test_cat))\n",
    "batch_size = 512\n",
    "\n",
    "# Prepare Test Loader\n",
    "test_ds = FounderDataset(X_test_cat, X_test_num)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size*2, shuffle=False)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_cat, y)):\n",
    "    print(f\"\\nâš¡ FOLD {fold+1}/{n_splits}\")\n",
    "    \n",
    "    # 1. Split\n",
    "    X_cat_tr, X_cat_val = X_cat[train_idx], X_cat[val_idx]\n",
    "    X_num_tr, X_num_val = X_num[train_idx], X_num[val_idx]\n",
    "    y_tr, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # 2. Loaders\n",
    "    train_ds = FounderDataset(X_cat_tr, X_num_tr, y_tr)\n",
    "    val_ds = FounderDataset(X_cat_val, X_num_val, y_val)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size*2, shuffle=False)\n",
    "    \n",
    "    # 3. Init Model with BEST PARAMS\n",
    "    model = GoatedMLP(\n",
    "        embedding_dims=cat_dims, \n",
    "        n_num=X_num.shape[1],\n",
    "        dropout=best_params['dropout'],\n",
    "        l1_size=best_params['l1_size'],\n",
    "        l2_size=best_params['l2_size'],\n",
    "        l3_size=best_params['l3_size']\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "    epochs = 40 # Increased epochs for final training\n",
    "    best_loss = float('inf')\n",
    "    patience = 7\n",
    "    counter = 0\n",
    "    best_weights = None\n",
    "    \n",
    "    # 4. Training Loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for cat, num, target in train_loader:\n",
    "            cat, num, target = cat.to(device), num.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(cat, num)\n",
    "            loss = criterion(logits, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for cat, num, target in val_loader:\n",
    "                cat, num, target = cat.to(device), num.to(device), target.to(device)\n",
    "                logits = model(cat, num)\n",
    "                val_loss += criterion(logits, target).item()\n",
    "        \n",
    "        avg_val = val_loss / len(val_loader)\n",
    "        scheduler.step(avg_val)\n",
    "        \n",
    "        if avg_val < best_loss:\n",
    "            best_loss = avg_val\n",
    "            best_weights = model.state_dict()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # 5. OOF & Test Predictions\n",
    "    model.load_state_dict(best_weights)\n",
    "    model.eval()\n",
    "    \n",
    "    # OOF\n",
    "    val_probs = []\n",
    "    with torch.no_grad():\n",
    "        for cat, num, _ in val_loader:\n",
    "            cat, num = cat.to(device), num.to(device)\n",
    "            logits = model(cat, num)\n",
    "            val_probs.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "    oof_preds[val_idx] = np.array(val_probs).flatten()\n",
    "    \n",
    "    # Test Accumulation\n",
    "    fold_test_probs = []\n",
    "    with torch.no_grad():\n",
    "        for cat, num in test_loader:\n",
    "            cat, num = cat.to(device), num.to(device)\n",
    "            logits = model(cat, num)\n",
    "            fold_test_probs.extend(torch.sigmoid(logits).cpu().numpy())\n",
    "    test_preds_accum += np.array(fold_test_probs).flatten() / n_splits\n",
    "    \n",
    "    fold_f1 = f1_score(y_val, (np.array(val_probs) >= 0.5).astype(int), average='macro')\n",
    "    print(f\"  Best Val Loss: {best_loss:.4f} | F1 (0.5): {fold_f1:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. SUBMISSION GENERATION\n",
    "# ============================================================================\n",
    "print(\"\\nOptimizing Threshold on OOF Predictions...\")\n",
    "best_f1 = 0\n",
    "best_thresh = 0.5\n",
    "for t in np.arange(0.3, 0.7, 0.001):\n",
    "    p = (oof_preds >= t).astype(int)\n",
    "    f1 = f1_score(y, p, average='macro')\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thresh = t\n",
    "\n",
    "print(f\"Global OOF Macro F1: {best_f1:.4f} at Threshold: {best_thresh:.3f}\")\n",
    "\n",
    "final_preds = (test_preds_accum >= best_thresh).astype(int)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "    'founder_id': test_ids,\n",
    "    'retention_status': ['Left' if p == 1 else 'Stayed' for p in final_preds]\n",
    "})\n",
    "sub.to_csv('submission_nn_optuna_final.csv', index=False)\n",
    "print(\"Saved 'submission_nn_optuna_final.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
